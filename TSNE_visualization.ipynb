{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851f430b",
   "metadata": {},
   "source": [
    "# A Notebook for Visualizing memory of trained sequencial policies\n",
    "\n",
    "Note that this codebase is made for evaluations in mujoco environments, therefore probably not compatible with other environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aef5ba",
   "metadata": {},
   "source": [
    "## Set log directory and import configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb972a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "class DotDict(dict):\n",
    "    \"\"\"d.a 처럼 접근 가능한 dict. 중첩도 재귀로 변환.\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "    def to_dict(self):\n",
    "        def _plain(x):\n",
    "            if isinstance(x, dict):\n",
    "                return {k: _plain(v) for k, v in x.items()}\n",
    "            if isinstance(x, list):\n",
    "                return [_plain(v) for v in x]\n",
    "            return x\n",
    "        return _plain(self)\n",
    "\n",
    "def to_dotdict(x):\n",
    "    if isinstance(x, dict):\n",
    "        return DotDict({k: to_dotdict(v) for k, v in x.items()})\n",
    "    if isinstance(x, list):\n",
    "        return [to_dotdict(v) for v in x]\n",
    "    return x\n",
    "\n",
    "def unwrap_value_nodes(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        if set(obj.keys()) == {\"value\"}:\n",
    "            return unwrap_value_nodes(obj[\"value\"])\n",
    "        return {k: unwrap_value_nodes(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        return [unwrap_value_nodes(x) for x in obj]\n",
    "    return obj\n",
    "\n",
    "def load_cfg(config_path):\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = yaml.safe_load(f)\n",
    "\n",
    "    raw = unwrap_value_nodes(raw)\n",
    "\n",
    "    # 필요한 3개만 추출\n",
    "    picked = {k: raw[k] for k in [\"config_env\", \"config_rl\", \"config_seq\"]}\n",
    "\n",
    "    # 점 접근 가능하게 변환\n",
    "    return to_dotdict(picked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f400feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "log_dir = \"SET_YOUR_LOG_DIR_HERE\" # Example: \"logs/mujoco/ant-dir/run_name_2026-01-05-17:27:32\"\n",
    "if log_dir == \"SET_YOUR_LOG_DIR_HERE\":\n",
    "    raise ValueError(\"Please set the 'log_dir' variable to your actual log directory path.\")\n",
    "\n",
    "config_dir = os.path.join(log_dir, \"wandb/latest-run/files/config.yaml\")\n",
    "\n",
    "\n",
    "cfg = load_cfg(config_dir)\n",
    "config_env = cfg.config_env\n",
    "config_rl = cfg.config_rl\n",
    "config_seq = cfg.config_seq\n",
    "print(\"Environment Config:\", config_env)\n",
    "print(\"RL Config:\", config_rl)\n",
    "print(\"Sequence Model Config:\", config_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78d8e39",
   "metadata": {},
   "source": [
    "## Make environment, Set seed and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f5444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register\n",
    "from torchkit.pytorch_utils import set_gpu_mode\n",
    "\n",
    "seed = 42\n",
    "gpu_id = 0\n",
    "device = f'cuda:{gpu_id}'\n",
    "set_gpu_mode(True, gpu_id)\n",
    "\n",
    "\n",
    "ENTRY_POINTS = {\"cheetah-vel\": \"envs.mujoco:HalfCheetahVelEnv\", \"ant-dir\": \"envs.mujoco:AntDirEnv\", \n",
    "                \"hopper-param\": \"envs.mujoco:HopperRandParamsEnv\", \"walker-param\": \"envs.mujoco:Walker2DRandParamsEnv\"}\n",
    "env_name = config_env.env_name\n",
    "entry_point = ENTRY_POINTS[env_name]\n",
    "register(\n",
    "    env_name,\n",
    "    entry_point=entry_point,\n",
    "    max_episode_steps=200,\n",
    "    kwargs=dict(terminate_when_unhealthy=config_env.terminate_when_unhealthy) if env_name not in [\"cheetah-vel\"] else {} # cheetah-vel does not have is_healthy\n",
    ")\n",
    "\n",
    "env = gym.make(env_name)\n",
    "env.max_episode_steps = getattr(\n",
    "    env, \"max_episode_steps\", env.spec.max_episode_steps\n",
    ")\n",
    "env.reset(seed=seed) # Set random seed\n",
    "env.action_space.seed(seed)\n",
    "env.observation_space.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71032235",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "act_dim = action_space.shape[0]\n",
    "obs_dim = observation_space.shape[0]\n",
    "\n",
    "print(\"obs space\", observation_space)\n",
    "print(\"act space\", action_space)\n",
    "print(\"obs_dim\", obs_dim, \"act_dim\", act_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aee38a2",
   "metadata": {},
   "source": [
    "## Instantiate agent and buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30315ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from policies.models.policy_rnn_shared import ModelFreeOffPolicy_Shared_RNN as Policy_Shared_RNN\n",
    "agent_class = Policy_Shared_RNN\n",
    "\n",
    "agent = agent_class(\n",
    "    obs_dim=obs_dim,\n",
    "    action_dim=act_dim,\n",
    "    config_seq=config_seq,\n",
    "    config_rl=config_rl,\n",
    "    freeze_critic=True,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "from buffers.rollout_buffer import RolloutBuffer\n",
    "buffer = RolloutBuffer(observation_dim=obs_dim,\n",
    "            action_dim=act_dim,\n",
    "            max_episode_len=env.max_episode_steps,\n",
    "            num_episodes=10000, # Not used in ICL testing\n",
    "            normalize_transitions=config_rl.normalize_transitions,\n",
    "            is_ppo=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91cba5",
   "metadata": {},
   "source": [
    "### Load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "agent_checkpoint_path = os.path.join(log_dir, \"policy_checkpoint_latest.pth\")\n",
    "agent.load_state_dict(torch.load(agent_checkpoint_path, map_location=device))\n",
    "if config_rl.normalize_transitions:\n",
    "    buffer_checkpoint_path = os.path.join(log_dir, \"buffer_checkpoint_latest.pth\")\n",
    "    buffer.load_state_dict(torch.load(buffer_checkpoint_path, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b59e1",
   "metadata": {},
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe80db55",
   "metadata": {},
   "source": [
    "### Define neccessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_dummies(env, obs):\n",
    "    prev_obs = obs.clone()\n",
    "    action = torch.FloatTensor([env.action_space.sample()]).to(device).reshape(1, -1)  # (1, A) for continuous action, (1, 1) for discrete action\n",
    "    reward = torch.zeros((1, 1)).to(device)\n",
    "    term = torch.zeros((1, 1)).to(device)\n",
    "    return prev_obs,action,reward,term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da23ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(internal_state, action, reward, prev_obs, obs, deterministic, initial):\n",
    "    if buffer.normalize_transitions:\n",
    "        obs = buffer.observation_rms.norm(obs)\n",
    "        prev_obs = buffer.observation_rms.norm(prev_obs)\n",
    "        reward = buffer.rewards_rms.norm(reward, scale=False)\n",
    "    action, internal_state = agent.act(\n",
    "        prev_internal_state=internal_state,\n",
    "        prev_action=action,\n",
    "        prev_reward=reward,\n",
    "        prev_obs=prev_obs,\n",
    "        obs=obs,\n",
    "        deterministic=deterministic,\n",
    "        initial=initial,\n",
    "    )\n",
    "    return action, internal_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff9bfaf",
   "metadata": {},
   "source": [
    "### Policy Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0540c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_rollouts = 500\n",
    "max_ep_len = env.max_episode_steps\n",
    "returns = []\n",
    "memories = []\n",
    "contexts = []\n",
    "\n",
    "print(\"Rollout Start\")\n",
    "print(\"Architecture:\", config_seq.seq_model.name)\n",
    "for rollout_idx in range(num_rollouts):\n",
    "    done = False\n",
    "    ep_return = 0.0\n",
    "    t = 0\n",
    "    obs, info = env.reset()\n",
    "    obs = torch.from_numpy(obs).float().to(device).unsqueeze(0) # (1, obs_dim)\n",
    "    prev_obs, action, reward, term = get_initial_dummies(env, obs)\n",
    "    internal_state = None\n",
    "    initial=True\n",
    "    \n",
    "    while not done and t < max_ep_len:\n",
    "        action, internal_state = act(\n",
    "            internal_state=internal_state,\n",
    "            action=action,\n",
    "            reward=reward,\n",
    "            prev_obs=prev_obs,\n",
    "            obs=obs,\n",
    "            deterministic=True,\n",
    "            initial=initial,\n",
    "        )\n",
    "        initial=False\n",
    "        np_action = action.to(\"cpu\").detach().numpy().squeeze(0) # (act_dim,)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(np_action)\n",
    "        ep_return += reward\n",
    "        next_obs = torch.from_numpy(next_obs).float().to(device).unsqueeze(0)  # (1, obs_dim)\n",
    "        reward = torch.tensor([[reward]], dtype=torch.float32).to(device)  # (1, 1)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        prev_obs = obs.clone()\n",
    "        obs = next_obs.clone()\n",
    "        t += 1\n",
    "\n",
    "    returns.append(ep_return)\n",
    "    hidden = agent.head.seq_model.internal_state_to_hidden(internal_state)[0][0] # (1, 1, hidden_size) -> (hidden_size,)\n",
    "    if config_seq.project_output:\n",
    "        hidden = hidden / torch.linalg.vector_norm(hidden).clamp(min=1e-6) * np.sqrt(len(hidden))\n",
    "    memories.append(hidden.detach().cpu().numpy())\n",
    "    contexts.append(info[\"context\"])\n",
    "    print(f\"Rollout {rollout_idx + 1}/{num_rollouts}, Return: {ep_return}\")\n",
    "\n",
    "returns = np.array(returns)\n",
    "memories = np.array(memories)  # (num_rollouts, hidden_size)\n",
    "contexts = np.array(contexts)  # (num_rollouts, context_dim)\n",
    "print(f\"Return over {num_rollouts} rollouts: avg {np.mean(returns)}, std {np.std(returns)}\")\n",
    "print(f\"memories shape: {memories.shape}, contexts shape: {contexts.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b292ed5f",
   "metadata": {},
   "source": [
    "### TSNE Visualization Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79af208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "name = f'tsne_{env_name}_{config_seq.seq_model.name}'\n",
    "\n",
    "# Initialize t-SNE (n_components=2 for 2D visualization)\n",
    "# random_state is recommended for reproducibility\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "# Fit and transform the data to the low-dimensional embedding\n",
    "memories_embedded = tsne.fit_transform(memories)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.scatter(\n",
    "    memories_embedded[:, 0],\n",
    "    memories_embedded[:, 1],\n",
    "    c=contexts[:, 0],      # 0th index of context is sufficient for cheetah-vel (May change based on environment)\n",
    "    s=12,                  # Marker size\n",
    ")\n",
    "\n",
    "# Add a colorbar to visualize the colormap scale\n",
    "plt.colorbar(label=\"context\")\n",
    "\n",
    "plt.title(name)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metaworld",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
