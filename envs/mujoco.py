import numpy as np
from gymnasium.envs.mujoco.half_cheetah_v5 import HalfCheetahEnv
from gymnasium.envs.mujoco.ant_v5 import AntEnv
from gymnasium.envs.mujoco.hopper_v5 import HopperEnv
from gymnasium.envs.mujoco.walker2d_v5 import Walker2dEnv


class HalfCheetahVelEnv(HalfCheetahEnv):
    """Half-cheetah environment with target velocity, as described in [1]. The
    code is adapted from
    https://github.com/cbfinn/maml_rl/blob/9c8e2ebd741cb0c7b8bf2d040c4caeeb8e06cc95/rllab/envs/mujoco/half_cheetah_env_rand.py

    The half-cheetah follows the dynamics from MuJoCo [2], and receives at each
    time step a reward composed of a control cost and a penalty equal to the
    difference between its current velocity and the target velocity. The tasks
    are generated by sampling the target velocities from the uniform
    distribution on [0, 3].

    [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic
        Meta-Learning for Fast Adaptation of Deep Networks", 2017
        (https://arxiv.org/abs/1703.03400)
    [2] Emanuel Todorov, Tom Erez, Yuval Tassa, "MuJoCo: A physics engine for
        model-based control", 2012
        (https://homes.cs.washington.edu/~todorov/papers/TodorovIROS12.pdf)
    """
    def __init__(self, **kwargs):
        super(HalfCheetahVelEnv, self).__init__(**kwargs)


    def _get_rew(self, x_velocity: float, action):
        forward_reward = - self._forward_reward_weight * abs(x_velocity - self._goal_vel)
        ctrl_cost = self.control_cost(action)
        reward = forward_reward - ctrl_cost

        reward_info = {
            "reward_forward": forward_reward,
            "reward_ctrl": -ctrl_cost,
            "context": np.array([self._goal])
        }
        return reward, reward_info
    

    def sample_goal(self):
        velocities = self.np_random.uniform(0.0, 3.0)
        return velocities


    def reset(self, **kwargs):
        obs, info = super().reset(**kwargs)
        self._goal_vel = self.sample_goal()
        self._goal = self._goal_vel
        info["context"] = np.array([self._goal])
        return obs, info


    def render(self):
        if self.render_mode is None:
            return None
        return super().render()



class AntDirEnv(AntEnv):

    def __init__(self, **kwargs):
        super(AntDirEnv, self).__init__(**kwargs)


    def step(self, action):
        xy_position_before = self.data.body(self._main_body).xpos[:2].copy()
        self.do_simulation(action, self.frame_skip)
        xy_position_after = self.data.body(self._main_body).xpos[:2].copy()

        xy_velocity = (xy_position_after - xy_position_before) / self.dt
        direct = (np.cos(self._goal), np.sin(self._goal))
        direct_velocity = np.dot(xy_velocity, direct)

        observation = self._get_obs()
        reward, reward_info = self._get_rew(direct_velocity, action)
        # terminated = (not self.is_healthy) and self._terminate_when_unhealthy
        terminated = False # No early termination for AntDirEnv
        info = {
            "x_position": self.data.qpos[0],
            "y_position": self.data.qpos[1],
            "distance_from_origin": np.linalg.norm(self.data.qpos[0:2], ord=2),
            "direct_velocity": direct_velocity,
            "context": np.array([self._goal]),
            **reward_info,
        }

        if self.render_mode == "human":
            self.render()
        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`
        return observation, reward, terminated, False, info
    

    def sample_goal(self):
        directions = self.np_random.uniform(0, 2.0 * np.pi)
        return directions
    
    def reset(self, **kwargs):
        obs, info = super().reset(**kwargs)
        self._goal_dir = self.sample_goal()
        self._goal = self._goal_dir
        info["context"] = np.array([self._goal])
        return obs, info

    def render(self):
        if self.render_mode is None:
            return None
        return super().render()


class HopperRandParamsEnv(HopperEnv):
    """
    Referenced from https://github.com/NJU-RL/Meta-DT/blob/main/src/tp_envs/rand_param_envs/hopper_rand_params.py
    Reimplemented for the latest mujoco and gymnasium versions.
    Modification : base for 'dof_damping' 1.3 -> 1.5 (Same value as the rest of the parameters)
    """

    RAND_PARAMS = ['body_mass', 'dof_damping', 'body_inertia', 'geom_friction']

    def __init__(self, *args, log_scale_limit: float = 3.0, base: float = 1.5, **kwargs):
        super().__init__(*args, **kwargs)
        self.log_scale_limit = float(log_scale_limit)
        self.rand_params = self.RAND_PARAMS
        self.base = base

        self._init_params = {}
        self._init_params['body_mass'] = self.model.body_mass.copy()
        self._init_params['body_inertia'] = self.model.body_inertia.copy()
        self._init_params['dof_damping'] = self.model.dof_damping.copy()
        self._init_params['geom_friction'] = self.model.geom_friction.copy()


    def _randomize_params(self):
        limit = self.log_scale_limit

        shape = self.model.body_mass.shape
        u = np.array([self.np_random.uniform(-limit, limit) for _ in range(np.prod(shape))]).reshape(shape)
        multipliers = (self.base ** u)
        self.model.body_mass[...] = self._init_params['body_mass'] * multipliers


        shape = self.model.body_inertia.shape
        u = np.array([self.np_random.uniform(-limit, limit) for _ in range(np.prod(shape))]).reshape(shape)
        multipliers = (self.base ** u)
        self.model.body_inertia[...] = self._init_params['body_inertia'] * multipliers


        shape = self.model.dof_damping.shape
        u = np.array([self.np_random.uniform(-limit, limit) for _ in range(np.prod(shape))]).reshape(shape)
        multipliers = (self.base ** u)
        self.model.dof_damping[...] = self._init_params['dof_damping'] * multipliers


        shape = self.model.geom_friction.shape
        u = np.array([self.np_random.uniform(-limit, limit) for _ in range(np.prod(shape))]).reshape(shape)
        multipliers = (self.base ** u)
        self.model.geom_friction[...] = self._init_params['geom_friction'] * multipliers

        current_params = {
            'body_mass': self.model.body_mass.copy(),
            'body_inertia': self.model.body_inertia.copy(),
            'dof_damping': self.model.dof_damping.copy(),
            'geom_friction': self.model.geom_friction.copy(),
            'context': np.concatenate([
                self.model.body_mass.copy().flatten(),
                self.model.body_inertia.copy().flatten(),
                self.model.dof_damping.copy().flatten(),
                self.model.geom_friction.copy().flatten()
            ])
        }
        return current_params


    def step(self, action):
        obs, reward, terminated, truncated, info = super().step(action)
        info.update(self.current_params)
        terminated = False # No early termination for HopperRandParamsEnv
        return obs, reward, terminated, truncated, info

    def reset(self, **kwargs):
        obs, info = super().reset(**kwargs)
        self.current_params = self._randomize_params()
        info.update(self.current_params)
        return obs, info

    def render(self):
        if self.render_mode is None:
            return None
        return super().render()



class Walker2DRandParamsEnv(Walker2dEnv):
    """
    Referenced from https://github.com/NJU-RL/Meta-DT/blob/main/src/tp_envs/rand_param_envs/walker2d_rand_params.py
    Reimplemented for the latest mujoco and gymnasium versions.
    Modification : base for 'dof_damping' 1.3 -> 1.5 (Same value as the rest of the parameters)
                   RAND_PARAMS = ['body_mass', 'geom_friction'] -> ['body_mass', 'dof_damping', 'body_inertia', 'geom_friction']
    """

    RAND_PARAMS = ['body_mass', 'dof_damping', 'body_inertia', 'geom_friction']

    def __init__(self, *args, log_scale_limit: float = 3.0, base: float = 1.5, **kwargs):
        super().__init__(*args, **kwargs)
        self.log_scale_limit = float(log_scale_limit)
        self.rand_params = self.RAND_PARAMS
        self.base = base

        self._init_params = {}
        self._init_params['body_mass'] = self.model.body_mass.copy()
        self._init_params['body_inertia'] = self.model.body_inertia.copy()
        self._init_params['dof_damping'] = self.model.dof_damping.copy()
        self._init_params['geom_friction'] = self.model.geom_friction.copy()


    def _randomize_params(self):
        limit = self.log_scale_limit

        shape = self.model.body_mass.shape
        u = np.array([self.np_random.uniform(-limit, limit) for _ in range(np.prod(shape))]).reshape(shape)
        multipliers = (self.base ** u)
        self.model.body_mass[...] = self._init_params['body_mass'] * multipliers


        shape = self.model.body_inertia.shape
        u = np.array([self.np_random.uniform(-limit, limit) for _ in range(np.prod(shape))]).reshape(shape)
        multipliers = (self.base ** u)
        self.model.body_inertia[...] = self._init_params['body_inertia'] * multipliers


        shape = self.model.dof_damping.shape
        u = np.array([self.np_random.uniform(-limit, limit) for _ in range(np.prod(shape))]).reshape(shape)
        multipliers = (self.base ** u)
        self.model.dof_damping[...] = self._init_params['dof_damping'] * multipliers


        shape = self.model.geom_friction.shape
        u = np.array([self.np_random.uniform(-limit, limit) for _ in range(np.prod(shape))]).reshape(shape)
        multipliers = (self.base ** u)
        self.model.geom_friction[...] = self._init_params['geom_friction'] * multipliers

        current_params = {
            'body_mass': self.model.body_mass.copy(),
            'body_inertia': self.model.body_inertia.copy(),
            'dof_damping': self.model.dof_damping.copy(),
            'geom_friction': self.model.geom_friction.copy(),
            'context': np.concatenate([
                self.model.body_mass.copy().flatten(),
                self.model.body_inertia.copy().flatten(),
                self.model.dof_damping.copy().flatten(),
                self.model.geom_friction.copy().flatten()
            ])
        }
        return current_params


    def step(self, action):
        obs, reward, terminated, truncated, info = super().step(action)
        info.update(self.current_params)
        terminated = False # No early termination for HopperRandParamsEnv
        return obs, reward, terminated, truncated, info

    def reset(self, **kwargs):
        obs, info = super().reset(**kwargs)
        self.current_params = self._randomize_params()
        info.update(self.current_params)
        return obs, info

    def render(self):
        if self.render_mode is None:
            return None
        return super().render()