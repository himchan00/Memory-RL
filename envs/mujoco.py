import numpy as np
from gymnasium.envs.mujoco.half_cheetah_v5 import HalfCheetahEnv
from gymnasium.envs.mujoco.ant_v5 import AntEnv
from gymnasium.envs.mujoco.hopper_v5 import HopperEnv
from gymnasium.envs.mujoco.walker2d_v5 import Walker2dEnv


# class HalfCheetahEnv(HalfCheetahEnv_):
#     def _get_obs(self):
#         return np.concatenate([
#             self.sim.data.qpos.flat[1:],
#             self.sim.data.qvel.flat,
#             self.get_body_com("torso").flat,
#         ]).astype(np.float32).flatten()

#     def viewer_setup(self):
#         camera_id = self.model.camera_name2id('track')
#         self.viewer.cam.type = 2
#         self.viewer.cam.fixedcamid = camera_id
#         self.viewer.cam.distance = self.model.stat.extent * 0.35
#         # Hide the overlay
#         self.viewer._hide_overlay = True

#     def render(self, mode='human'):
#         if mode == 'rgb_array':
#             self._get_viewer(mode).render()
#             # window size used for old mujoco-py:
#             width, height = 500, 500
#             data = self._get_viewer().read_pixels(width, height, depth=False)
#             return data
#         elif mode == 'human':
#             self._get_viewer(mode).render()

class HalfCheetahVelEnv(HalfCheetahEnv):
    """Half-cheetah environment with target velocity, as described in [1]. The
    code is adapted from
    https://github.com/cbfinn/maml_rl/blob/9c8e2ebd741cb0c7b8bf2d040c4caeeb8e06cc95/rllab/envs/mujoco/half_cheetah_env_rand.py

    The half-cheetah follows the dynamics from MuJoCo [2], and receives at each
    time step a reward composed of a control cost and a penalty equal to the
    difference between its current velocity and the target velocity. The tasks
    are generated by sampling the target velocities from the uniform
    distribution on [0, 3].

    [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic
        Meta-Learning for Fast Adaptation of Deep Networks", 2017
        (https://arxiv.org/abs/1703.03400)
    [2] Emanuel Todorov, Tom Erez, Yuval Tassa, "MuJoCo: A physics engine for
        model-based control", 2012
        (https://homes.cs.washington.edu/~todorov/papers/TodorovIROS12.pdf)
    """
    def __init__(self, render_mode: str | None = None, **kwargs):
        # forward render_mode (and any other kwargs) to the parent mujoco env
        super().__init__(render_mode=render_mode, **kwargs)


    def _get_rew(self, x_velocity: float, action):
        forward_reward = - self._forward_reward_weight * abs(x_velocity - self._goal_vel)
        ctrl_cost = self.control_cost(action)
        reward = forward_reward - ctrl_cost

        reward_info = {
            "reward_forward": forward_reward,
            "reward_ctrl": -ctrl_cost,
            "goal": self._goal
        }
        return reward, reward_info
    

    def sample_goal(self):
        velocities = self.np_random.uniform(0.0, 3.0)
        return velocities

    # def reset(self, **kwargs):
    #     self._goal_vel = self.sample_goal()
    #     self._goal = self._goal_vel
    #     return super().reset(**kwargs)
    
    def reset(self, *, seed=None, options=None):
        # Seed RNG and reset mujoco state first
        obs, info = super().reset(seed=seed, options=options)

        # Now sample per-episode goal using the freshly seeded RNG
        self._goal_vel = self.sample_goal()
        self._goal = self._goal_vel

        # (optional) expose goal in info
        info = dict(info)
        info["goal"] = self._goal
        return obs, info


# class HalfCheetahDirEnv(HalfCheetahEnv):
#     """Half-cheetah environment with target direction, as described in [1]. The
#     code is adapted from
#     https://github.com/cbfinn/maml_rl/blob/9c8e2ebd741cb0c7b8bf2d040c4caeeb8e06cc95/rllab/envs/mujoco/half_cheetah_env_rand_direc.py

#     The half-cheetah follows the dynamics from MuJoCo [2], and receives at each
#     time step a reward composed of a control cost and a reward equal to its
#     velocity in the target direction. The tasks are generated by sampling the
#     target directions from a Bernoulli distribution on {-1, 1} with parameter
#     0.5 (-1: backward, +1: forward).

#     [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic
#         Meta-Learning for Fast Adaptation of Deep Networks", 2017
#         (https://arxiv.org/abs/1703.03400)
#     [2] Emanuel Todorov, Tom Erez, Yuval Tassa, "MuJoCo: A physics engine for
#         model-based control", 2012
#         (https://homes.cs.washington.edu/~todorov/papers/TodorovIROS12.pdf)
#     """
#     def __init__(self):
#         super(HalfCheetahDirEnv, self).__init__()

#     def _get_rew(self, x_velocity: float, action):
#         forward_reward = self._goal_dir * self._forward_reward_weight * x_velocity
#         ctrl_cost = self.control_cost(action)

#         reward = forward_reward - ctrl_cost

#         reward_info = {
#             "reward_forward": forward_reward,
#             "reward_ctrl": -ctrl_cost,
#             "goal": self._goal
#         }
#         return reward, reward_info
    
#     def sample_goal(self):
#         directions = 2 * self.np_random.binomial(1, p=0.5) - 1
#         return directions

    
#     def reset(self, **kwargs):
#         self._goal_dir = self.sample_goal()
#         self._goal = self._goal_dir
#         return super().reset(**kwargs)



class AntDirEnv(AntEnv):

    def __init__(self):
        super(AntDirEnv, self).__init__()


    def step(self, action):
        xy_position_before = self.data.body(self._main_body).xpos[:2].copy()
        self.do_simulation(action, self.frame_skip)
        xy_position_after = self.data.body(self._main_body).xpos[:2].copy()

        xy_velocity = (xy_position_after - xy_position_before) / self.dt
        direct = (np.cos(self._goal), np.sin(self._goal))
        direct_velocity = np.dot(xy_velocity, direct)

        observation = self._get_obs()
        reward, reward_info = self._get_rew(direct_velocity, action)
        # terminated = (not self.is_healthy) and self._terminate_when_unhealthy
        terminated = False # No early termination for AntDirEnv
        info = {
            "x_position": self.data.qpos[0],
            "y_position": self.data.qpos[1],
            "distance_from_origin": np.linalg.norm(self.data.qpos[0:2], ord=2),
            "direct_velocity": direct_velocity,
            "goal": self._goal,
            **reward_info,
        }

        if self.render_mode == "human":
            self.render()
        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`
        return observation, reward, terminated, False, info
    

    def sample_goal(self):
        directions = self.np_random.uniform(0, 2.0 * np.pi)
        return directions
    
    def reset(self, **kwargs):
        self._goal_dir = self.sample_goal()
        self._goal = self._goal_dir
        return super().reset(**kwargs)
    



class HopperRandParamsEnv(HopperEnv):
    """
    Referenced from https://github.com/NJU-RL/Meta-DT/blob/main/src/tp_envs/rand_param_envs/hopper_rand_params.py
    Reimplemented for the latest mujoco and gymnasium versions.
    Modification : base for 'dof_damping' 1.3 -> 1.5 (Same value as the rest of the parameters)
    """

    RAND_PARAMS = ['body_mass', 'dof_damping', 'body_inertia', 'geom_friction']

    def __init__(self, *args, log_scale_limit: float = 3.0, base: float = 1.5, **kwargs):
        super().__init__(*args, **kwargs)
        self.log_scale_limit = float(log_scale_limit)
        self.rand_params = self.RAND_PARAMS
        self.base = base

        self._init_params = {}
        self._init_params['body_mass'] = self.model.body_mass.copy()
        self._init_params['body_inertia'] = self.model.body_inertia.copy()
        self._init_params['dof_damping'] = self.model.dof_damping.copy()
        self._init_params['geom_friction'] = self.model.geom_friction.copy()


    def _randomize_params(self):
        limit = self.log_scale_limit

        shape = self.model.body_mass.shape
        u = np.array([self.np_random.uniform(-limit, limit) for _ in range(np.prod(shape))]).reshape(shape)
        multipliers = (self.base ** u)
        self.model.body_mass[...] = self._init_params['body_mass'] * multipliers


        shape = self.model.body_inertia.shape
        u = np.array([self.np_random.uniform(-limit, limit) for _ in range(np.prod(shape))]).reshape(shape)
        multipliers = (self.base ** u)
        self.model.body_inertia[...] = self._init_params['body_inertia'] * multipliers


        shape = self.model.dof_damping.shape
        u = np.array([self.np_random.uniform(-limit, limit) for _ in range(np.prod(shape))]).reshape(shape)
        multipliers = (self.base ** u)
        self.model.dof_damping[...] = self._init_params['dof_damping'] * multipliers


        shape = self.model.geom_friction.shape
        u = np.array([self.np_random.uniform(-limit, limit) for _ in range(np.prod(shape))]).reshape(shape)
        multipliers = (self.base ** u)
        self.model.geom_friction[...] = self._init_params['geom_friction'] * multipliers

        current_params = {
            'body_mass': self.model.body_mass.copy(),
            'body_inertia': self.model.body_inertia.copy(),
            'dof_damping': self.model.dof_damping.copy(),
            'geom_friction': self.model.geom_friction.copy(),
        }
        return current_params


    def step(self, action):
        obs, reward, terminated, truncated, info = super().step(action)
        info.update(self.current_params)
        terminated = False # No early termination for HopperRandParamsEnv
        return obs, reward, terminated, truncated, info

    def reset(self, **kwargs):
        self.current_params = self._randomize_params()
        obs, info = super().reset(**kwargs)
        info.update(self.current_params)
        return obs, info
    


class Walker2DRandParamsEnv(Walker2dEnv):
    """
    Referenced from https://github.com/NJU-RL/Meta-DT/blob/main/src/tp_envs/rand_param_envs/walker2d_rand_params.py
    Reimplemented for the latest mujoco and gymnasium versions.
    Modification : base for 'dof_damping' 1.3 -> 1.5 (Same value as the rest of the parameters)
                   RAND_PARAMS = ['body_mass', 'geom_friction'] -> ['body_mass', 'dof_damping', 'body_inertia', 'geom_friction']
    """

    RAND_PARAMS = ['body_mass', 'dof_damping', 'body_inertia', 'geom_friction']

    def __init__(self, *args, log_scale_limit: float = 3.0, base: float = 1.5, **kwargs):
        super().__init__(*args, **kwargs)
        self.log_scale_limit = float(log_scale_limit)
        self.rand_params = self.RAND_PARAMS
        self.base = base

        self._init_params = {}
        self._init_params['body_mass'] = self.model.body_mass.copy()
        self._init_params['body_inertia'] = self.model.body_inertia.copy()
        self._init_params['dof_damping'] = self.model.dof_damping.copy()
        self._init_params['geom_friction'] = self.model.geom_friction.copy()


    def _randomize_params(self):
        limit = self.log_scale_limit

        shape = self.model.body_mass.shape
        u = np.array([self.np_random.uniform(-limit, limit) for _ in range(np.prod(shape))]).reshape(shape)
        multipliers = (self.base ** u)
        self.model.body_mass[...] = self._init_params['body_mass'] * multipliers


        shape = self.model.body_inertia.shape
        u = np.array([self.np_random.uniform(-limit, limit) for _ in range(np.prod(shape))]).reshape(shape)
        multipliers = (self.base ** u)
        self.model.body_inertia[...] = self._init_params['body_inertia'] * multipliers


        shape = self.model.dof_damping.shape
        u = np.array([self.np_random.uniform(-limit, limit) for _ in range(np.prod(shape))]).reshape(shape)
        multipliers = (self.base ** u)
        self.model.dof_damping[...] = self._init_params['dof_damping'] * multipliers


        shape = self.model.geom_friction.shape
        u = np.array([self.np_random.uniform(-limit, limit) for _ in range(np.prod(shape))]).reshape(shape)
        multipliers = (self.base ** u)
        self.model.geom_friction[...] = self._init_params['geom_friction'] * multipliers

        current_params = {
            'body_mass': self.model.body_mass.copy(),
            'body_inertia': self.model.body_inertia.copy(),
            'dof_damping': self.model.dof_damping.copy(),
            'geom_friction': self.model.geom_friction.copy(),
        }
        return current_params


    def step(self, action):
        obs, reward, terminated, truncated, info = super().step(action)
        info.update(self.current_params)
        terminated = False # No early termination for HopperRandParamsEnv
        return obs, reward, terminated, truncated, info

    def reset(self, **kwargs):
        self.current_params = self._randomize_params()
        obs, info = super().reset(**kwargs)
        info.update(self.current_params)
        return obs, info