import numpy as np
from gymnasium.envs.mujoco.half_cheetah_v5 import HalfCheetahEnv
from gymnasium.envs.mujoco.ant_v5 import AntEnv


# class HalfCheetahEnv(HalfCheetahEnv_):
#     def _get_obs(self):
#         return np.concatenate([
#             self.sim.data.qpos.flat[1:],
#             self.sim.data.qvel.flat,
#             self.get_body_com("torso").flat,
#         ]).astype(np.float32).flatten()

#     def viewer_setup(self):
#         camera_id = self.model.camera_name2id('track')
#         self.viewer.cam.type = 2
#         self.viewer.cam.fixedcamid = camera_id
#         self.viewer.cam.distance = self.model.stat.extent * 0.35
#         # Hide the overlay
#         self.viewer._hide_overlay = True

#     def render(self, mode='human'):
#         if mode == 'rgb_array':
#             self._get_viewer(mode).render()
#             # window size used for old mujoco-py:
#             width, height = 500, 500
#             data = self._get_viewer().read_pixels(width, height, depth=False)
#             return data
#         elif mode == 'human':
#             self._get_viewer(mode).render()

class HalfCheetahVelEnv(HalfCheetahEnv):
    """Half-cheetah environment with target velocity, as described in [1]. The
    code is adapted from
    https://github.com/cbfinn/maml_rl/blob/9c8e2ebd741cb0c7b8bf2d040c4caeeb8e06cc95/rllab/envs/mujoco/half_cheetah_env_rand.py

    The half-cheetah follows the dynamics from MuJoCo [2], and receives at each
    time step a reward composed of a control cost and a penalty equal to the
    difference between its current velocity and the target velocity. The tasks
    are generated by sampling the target velocities from the uniform
    distribution on [0, 3].

    [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic
        Meta-Learning for Fast Adaptation of Deep Networks", 2017
        (https://arxiv.org/abs/1703.03400)
    [2] Emanuel Todorov, Tom Erez, Yuval Tassa, "MuJoCo: A physics engine for
        model-based control", 2012
        (https://homes.cs.washington.edu/~todorov/papers/TodorovIROS12.pdf)
    """
    def __init__(self):
        super(HalfCheetahVelEnv, self).__init__()


    def _get_rew(self, x_velocity: float, action):
        forward_reward = - self._forward_reward_weight * abs(x_velocity - self._goal_vel)
        ctrl_cost = self.control_cost(action)
        reward = forward_reward - ctrl_cost

        reward_info = {
            "reward_forward": forward_reward,
            "reward_ctrl": -ctrl_cost,
            "goal": self._goal_vel
        }
        return reward, reward_info
    

    def sample_goal(self):
        velocities = self.np_random.uniform(0.0, 3.0)
        return velocities

    def reset(self, **kwargs):
        self._goal_vel = self.sample_goal()
        self._goal = self._goal_vel
        return super().reset(**kwargs)

class HalfCheetahDirEnv(HalfCheetahEnv):
    """Half-cheetah environment with target direction, as described in [1]. The
    code is adapted from
    https://github.com/cbfinn/maml_rl/blob/9c8e2ebd741cb0c7b8bf2d040c4caeeb8e06cc95/rllab/envs/mujoco/half_cheetah_env_rand_direc.py

    The half-cheetah follows the dynamics from MuJoCo [2], and receives at each
    time step a reward composed of a control cost and a reward equal to its
    velocity in the target direction. The tasks are generated by sampling the
    target directions from a Bernoulli distribution on {-1, 1} with parameter
    0.5 (-1: backward, +1: forward).

    [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic
        Meta-Learning for Fast Adaptation of Deep Networks", 2017
        (https://arxiv.org/abs/1703.03400)
    [2] Emanuel Todorov, Tom Erez, Yuval Tassa, "MuJoCo: A physics engine for
        model-based control", 2012
        (https://homes.cs.washington.edu/~todorov/papers/TodorovIROS12.pdf)
    """
    def __init__(self):
        super(HalfCheetahDirEnv, self).__init__()

    def _get_rew(self, x_velocity: float, action):
        forward_reward = self._goal_dir * self._forward_reward_weight * x_velocity
        ctrl_cost = self.control_cost(action)

        reward = forward_reward - ctrl_cost

        reward_info = {
            "reward_forward": forward_reward,
            "reward_ctrl": -ctrl_cost,
            "goal": self._goal_dir
        }
        return reward, reward_info
    
    def sample_goal(self):
        directions = 2 * self.np_random.binomial(1, p=0.5) - 1
        return directions

    
    def reset(self, **kwargs):
        self._goal_dir = self.sample_goal()
        self._goal = self._goal_dir
        return super().reset(**kwargs)



class AntDirEnv(AntEnv):

    def __init__(self):
        super(AntDirEnv, self).__init__()


    def step(self, action):
        xy_position_before = self.data.body(self._main_body).xpos[:2].copy()
        self.do_simulation(action, self.frame_skip)
        xy_position_after = self.data.body(self._main_body).xpos[:2].copy()

        xy_velocity = (xy_position_after - xy_position_before) / self.dt
        direct = (np.cos(self._goal), np.sin(self._goal))
        direct_velocity = np.dot(xy_velocity, direct)

        observation = self._get_obs()
        reward, reward_info = self._get_rew(direct_velocity, action)
        # terminated = (not self.is_healthy) and self._terminate_when_unhealthy
        terminated = False # No early termination for AntDirEnv
        info = {
            "x_position": self.data.qpos[0],
            "y_position": self.data.qpos[1],
            "distance_from_origin": np.linalg.norm(self.data.qpos[0:2], ord=2),
            "direct_velocity": direct_velocity,
            **reward_info,
        }

        if self.render_mode == "human":
            self.render()
        # truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`
        return observation, reward, terminated, False, info
    

    def sample_goal(self):
        directions = self.np_random.uniform(0, 2.0 * np.pi)
        return directions
    
    def reset(self, **kwargs):
        self._goal_dir = self.sample_goal()
        self._goal = self._goal_dir
        return super().reset(**kwargs)