import numpy as np
from gymnasium.envs.mujoco.half_cheetah_v5 import HalfCheetahEnv


# class HalfCheetahEnv(HalfCheetahEnv_):
#     def _get_obs(self):
#         return np.concatenate([
#             self.sim.data.qpos.flat[1:],
#             self.sim.data.qvel.flat,
#             self.get_body_com("torso").flat,
#         ]).astype(np.float32).flatten()

#     def viewer_setup(self):
#         camera_id = self.model.camera_name2id('track')
#         self.viewer.cam.type = 2
#         self.viewer.cam.fixedcamid = camera_id
#         self.viewer.cam.distance = self.model.stat.extent * 0.35
#         # Hide the overlay
#         self.viewer._hide_overlay = True

#     def render(self, mode='human'):
#         if mode == 'rgb_array':
#             self._get_viewer(mode).render()
#             # window size used for old mujoco-py:
#             width, height = 500, 500
#             data = self._get_viewer().read_pixels(width, height, depth=False)
#             return data
#         elif mode == 'human':
#             self._get_viewer(mode).render()

class HalfCheetahVelEnv(HalfCheetahEnv):
    """Half-cheetah environment with target velocity, as described in [1]. The
    code is adapted from
    https://github.com/cbfinn/maml_rl/blob/9c8e2ebd741cb0c7b8bf2d040c4caeeb8e06cc95/rllab/envs/mujoco/half_cheetah_env_rand.py

    The half-cheetah follows the dynamics from MuJoCo [2], and receives at each
    time step a reward composed of a control cost and a penalty equal to the
    difference between its current velocity and the target velocity. The tasks
    are generated by sampling the target velocities from the uniform
    distribution on [0, 2].

    [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic
        Meta-Learning for Fast Adaptation of Deep Networks", 2017
        (https://arxiv.org/abs/1703.03400)
    [2] Emanuel Todorov, Tom Erez, Yuval Tassa, "MuJoCo: A physics engine for
        model-based control", 2012
        (https://homes.cs.washington.edu/~todorov/papers/TodorovIROS12.pdf)
    """
    def __init__(self):
        super(HalfCheetahVelEnv, self).__init__()


    def _get_rew(self, x_velocity: float, action):
        forward_reward = - self._forward_reward_weight * abs(x_velocity - self._goal_vel)
        ctrl_cost = self.control_cost(action)
        reward = forward_reward - ctrl_cost

        reward_info = {
            "reward_forward": forward_reward,
            "reward_ctrl": -ctrl_cost,
            "goal": self._goal_vel
        }
        return reward, reward_info
    

    def sample_goal(self):
        velocities = self.np_random.uniform(0.0, 3.0)
        return velocities

    def reset(self, **kwargs):
        self._goal_vel = self.sample_goal()
        self._goal = self._goal_vel
        return super().reset(**kwargs)

class HalfCheetahDirEnv(HalfCheetahEnv):
    """Half-cheetah environment with target direction, as described in [1]. The
    code is adapted from
    https://github.com/cbfinn/maml_rl/blob/9c8e2ebd741cb0c7b8bf2d040c4caeeb8e06cc95/rllab/envs/mujoco/half_cheetah_env_rand_direc.py

    The half-cheetah follows the dynamics from MuJoCo [2], and receives at each
    time step a reward composed of a control cost and a reward equal to its
    velocity in the target direction. The tasks are generated by sampling the
    target directions from a Bernoulli distribution on {-1, 1} with parameter
    0.5 (-1: backward, +1: forward).

    [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic
        Meta-Learning for Fast Adaptation of Deep Networks", 2017
        (https://arxiv.org/abs/1703.03400)
    [2] Emanuel Todorov, Tom Erez, Yuval Tassa, "MuJoCo: A physics engine for
        model-based control", 2012
        (https://homes.cs.washington.edu/~todorov/papers/TodorovIROS12.pdf)
    """
    def __init__(self):
        super(HalfCheetahDirEnv, self).__init__()

    def _get_rew(self, x_velocity: float, action):
        forward_reward = self._goal_dir * self._forward_reward_weight * x_velocity
        ctrl_cost = self.control_cost(action)

        reward = forward_reward - ctrl_cost

        reward_info = {
            "reward_forward": forward_reward,
            "reward_ctrl": -ctrl_cost,
            "goal": self._goal_dir
        }
        return reward, reward_info
    
    def sample_goal(self):
        directions = 2 * self.np_random.binomial(1, p=0.5) - 1
        return directions

    
    def reset(self, **kwargs):
        self._goal_dir = self.sample_goal()
        self._goal = self._goal_dir
        return super().reset(**kwargs)

