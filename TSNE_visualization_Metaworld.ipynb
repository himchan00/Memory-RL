{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851f430b",
   "metadata": {},
   "source": [
    "# A Notebook for Visualizing memory of trained sequencial policies\n",
    "\n",
    "Note that this codebase is made for evaluations in mujoco environments, therefore probably not compatible with other environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aef5ba",
   "metadata": {},
   "source": [
    "## Set log directory and import configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb972a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "class DotDict(dict):\n",
    "    \"\"\"d.a 처럼 접근 가능한 dict. 중첩도 재귀로 변환.\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "    def to_dict(self):\n",
    "        def _plain(x):\n",
    "            if isinstance(x, dict):\n",
    "                return {k: _plain(v) for k, v in x.items()}\n",
    "            if isinstance(x, list):\n",
    "                return [_plain(v) for v in x]\n",
    "            return x\n",
    "        return _plain(self)\n",
    "\n",
    "def to_dotdict(x):\n",
    "    if isinstance(x, dict):\n",
    "        return DotDict({k: to_dotdict(v) for k, v in x.items()})\n",
    "    if isinstance(x, list):\n",
    "        return [to_dotdict(v) for v in x]\n",
    "    return x\n",
    "\n",
    "def unwrap_value_nodes(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        if set(obj.keys()) == {\"value\"}:\n",
    "            return unwrap_value_nodes(obj[\"value\"])\n",
    "        return {k: unwrap_value_nodes(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        return [unwrap_value_nodes(x) for x in obj]\n",
    "    return obj\n",
    "\n",
    "def load_cfg(config_path):\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = yaml.safe_load(f)\n",
    "\n",
    "    raw = unwrap_value_nodes(raw)\n",
    "\n",
    "    # 필요한 3개만 추출\n",
    "    picked = {k: raw[k] for k in [\"config_env\", \"config_rl\", \"config_seq\"]}\n",
    "\n",
    "    # 점 접근 가능하게 변환\n",
    "    return to_dotdict(picked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f400feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "log_dir = \"logs/Metaworld/ML10/mate_0_2026-01-24-15:11:44\" # Example: \"logs/mujoco/ant-dir/run_name_2026-01-05-17:27:32\"\n",
    "if log_dir == \"SET_YOUR_LOG_DIR_HERE\":\n",
    "    raise ValueError(\"Please set the 'log_dir' variable to your actual log directory path.\")\n",
    "\n",
    "config_dir = os.path.join(log_dir, \"wandb/latest-run/files/config.yaml\")\n",
    "\n",
    "\n",
    "cfg = load_cfg(config_dir)\n",
    "config_env = cfg.config_env\n",
    "config_rl = cfg.config_rl\n",
    "config_seq = cfg.config_seq\n",
    "print(\"Environment Config:\", config_env)\n",
    "print(\"RL Config:\", config_rl)\n",
    "print(\"Sequence Model Config:\", config_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78d8e39",
   "metadata": {},
   "source": [
    "## Make environment, Set seed and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f5444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register\n",
    "from envs.make_env import MLWrapper\n",
    "from torchkit.pytorch_utils import set_gpu_mode\n",
    "\n",
    "seed = 42\n",
    "gpu_id = 1\n",
    "device = f'cuda:{gpu_id}'\n",
    "set_gpu_mode(True, gpu_id)\n",
    "\n",
    "env_name = config_env.env_name\n",
    "\n",
    "env = MLWrapper(env_name, mode=\"train\", render_mode=\"rgb_array\")\n",
    "\n",
    "env.reset(seed=seed) # Set random seed\n",
    "env.action_space.seed(seed)\n",
    "env.observation_space.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71032235",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "act_dim = action_space.shape[0]\n",
    "obs_dim = observation_space.shape[0]\n",
    "\n",
    "print(\"obs space\", observation_space)\n",
    "print(\"act space\", action_space)\n",
    "print(\"obs_dim\", obs_dim, \"act_dim\", act_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aee38a2",
   "metadata": {},
   "source": [
    "## Instantiate agent and buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30315ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from policies.models.policy_rnn_shared import ModelFreeOffPolicy_Shared_RNN as Policy_Shared_RNN\n",
    "agent_class = Policy_Shared_RNN\n",
    "\n",
    "agent = agent_class(\n",
    "    obs_dim=obs_dim,\n",
    "    action_dim=act_dim,\n",
    "    config_seq=config_seq,\n",
    "    config_rl=config_rl,\n",
    "    freeze_critic=True,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "from buffers.rollout_buffer import RolloutBuffer\n",
    "buffer = RolloutBuffer(observation_dim=obs_dim,\n",
    "            action_dim=act_dim,\n",
    "            max_episode_len=env.env.max_episode_steps,\n",
    "            num_episodes=10000, # Not used in ICL testing\n",
    "            normalize_transitions=config_env.normalize_transitions,\n",
    "            is_ppo=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91cba5",
   "metadata": {},
   "source": [
    "### Load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "agent_checkpoint_path = os.path.join(log_dir, \"policy_checkpoint_latest.pth\")\n",
    "agent.load_state_dict(torch.load(agent_checkpoint_path, map_location=device))\n",
    "if config_env.normalize_transitions:\n",
    "    buffer_checkpoint_path = os.path.join(log_dir, \"buffer_checkpoint_latest.pth\")\n",
    "    buffer.load_state_dict(torch.load(buffer_checkpoint_path, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b59e1",
   "metadata": {},
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe80db55",
   "metadata": {},
   "source": [
    "### Define neccessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_dummies(env, obs):\n",
    "    prev_obs = obs.clone()\n",
    "    action = torch.FloatTensor([env.action_space.sample()]).to(device).reshape(1, -1)  # (1, A) for continuous action, (1, 1) for discrete action\n",
    "    reward = torch.zeros((1, 1)).to(device)\n",
    "    term = torch.zeros((1, 1)).to(device)\n",
    "    return prev_obs,action,reward,term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da23ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(internal_state, action, reward, prev_obs, obs, deterministic, initial):\n",
    "    if buffer.normalize_transitions:\n",
    "        obs = buffer.observation_rms.norm(obs)\n",
    "        prev_obs = buffer.observation_rms.norm(prev_obs)\n",
    "        reward = buffer.rewards_rms.norm(reward, scale=False)\n",
    "    action, internal_state = agent.act(\n",
    "        prev_internal_state=internal_state,\n",
    "        prev_action=action,\n",
    "        prev_reward=reward,\n",
    "        prev_obs=prev_obs,\n",
    "        obs=obs,\n",
    "        deterministic=deterministic,\n",
    "        initial=initial,\n",
    "    )\n",
    "    return action, internal_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff9bfaf",
   "metadata": {},
   "source": [
    "### Policy Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0540c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_rollouts = 1000\n",
    "max_ep_len = env.env.max_episode_steps\n",
    "returns = []\n",
    "memories = []\n",
    "name_to_idx = {name: idx for idx, name in enumerate(env.classes.keys())}\n",
    "contexts = []\n",
    "\n",
    "\n",
    "print(\"Rollout Start\")\n",
    "print(\"Architecture:\", config_seq.seq_model.name)\n",
    "for rollout_idx in range(num_rollouts):\n",
    "    done = False\n",
    "    ep_return = 0.0\n",
    "    t = 0\n",
    "    obs, info = env.reset()\n",
    "    obs = torch.from_numpy(obs).float().to(device).unsqueeze(0) # (1, obs_dim)\n",
    "    prev_obs, action, reward, term = get_initial_dummies(env, obs)\n",
    "    internal_state = None\n",
    "    initial=True\n",
    "    \n",
    "    while not done and t < max_ep_len:\n",
    "        action, internal_state = act(\n",
    "            internal_state=internal_state,\n",
    "            action=action,\n",
    "            reward=reward,\n",
    "            prev_obs=prev_obs,\n",
    "            obs=obs,\n",
    "            deterministic=True,\n",
    "            initial=initial,\n",
    "        )\n",
    "        initial=False\n",
    "        np_action = action.to(\"cpu\").detach().numpy().squeeze(0) # (act_dim,)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(np_action)\n",
    "        ep_return += reward\n",
    "        next_obs = torch.from_numpy(next_obs).float().to(device).unsqueeze(0)  # (1, obs_dim)\n",
    "        reward = torch.tensor([[reward]], dtype=torch.float32).to(device)  # (1, 1)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        prev_obs = obs.clone()\n",
    "        obs = next_obs.clone()\n",
    "        t += 1\n",
    "\n",
    "    returns.append(ep_return)\n",
    "    hidden = agent.head.seq_model.internal_state_to_hidden(internal_state)[0][0] # (1, 1, hidden_size) -> (hidden_size,)\n",
    "    if config_seq.project_output:\n",
    "        hidden = hidden / torch.linalg.vector_norm(hidden).clamp(min=1e-6) * np.sqrt(len(hidden))\n",
    "    memories.append(hidden.detach().cpu().numpy())\n",
    "    contexts.append(name_to_idx[info[\"name\"]])\n",
    "    print(f\"Rollout {rollout_idx + 1}/{num_rollouts}, Return: {ep_return}\")\n",
    "\n",
    "returns = np.array(returns)\n",
    "memories = np.array(memories)  # (num_rollouts, hidden_size)\n",
    "contexts = np.array(contexts)  # (num_rollouts, context_dim)\n",
    "print(f\"Return over {num_rollouts} rollouts: avg {np.mean(returns)}, std {np.std(returns)}\")\n",
    "print(f\"memories shape: {memories.shape}, contexts shape: {contexts.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b292ed5f",
   "metadata": {},
   "source": [
    "### TSNE Visualization Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79af208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# idx -> name\n",
    "idx_to_name = {v: k for k, v in name_to_idx.items()}\n",
    "memories_plot = []\n",
    "contexts_plot = []\n",
    "indices = [2, 5, 3]\n",
    "for idx in indices:\n",
    "    name = idx_to_name[idx]\n",
    "    print(f\"Index {idx} -> Name: {name}\")\n",
    "    print(\"  -> Included in the plot.\")\n",
    "    memories_plot.append(memories[contexts == idx])\n",
    "    contexts_plot.append(np.full((np.sum(contexts == idx),), idx))\n",
    "\n",
    "memories_plot = np.vstack(memories_plot)\n",
    "contexts_plot = np.concatenate(contexts_plot)\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "memories_embedded = tsne.fit_transform(memories_plot)\n",
    "\n",
    "\n",
    "# --- font settings ---\n",
    "FONT = \"Calibri\"   # 없으면 알아서 fallback 됨(환경에 따라)\n",
    "TICK_SIZE = 20\n",
    "LABEL_SIZE = 20\n",
    "LEGEND_SIZE = 28\n",
    "LEGEND_TITLE_SIZE =28\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Use categorical colormap with enough distinct colors for 5 classes\n",
    "cmap = plt.get_cmap(\"tab10\")\n",
    "\n",
    "for ctx in indices:\n",
    "    mask = (contexts_plot == ctx)\n",
    "    ax.scatter(\n",
    "        memories_embedded[mask, 0],\n",
    "        memories_embedded[mask, 1],\n",
    "        s=20,\n",
    "        alpha=0.8,\n",
    "        color=cmap(int(ctx) % 5),\n",
    "        label=idx_to_name.get(int(ctx), str(ctx)).removesuffix(\"-v3\").removesuffix(\"-topdown\"),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "ax.tick_params(axis=\"both\", labelsize=TICK_SIZE)\n",
    "for t in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "    t.set_fontname(FONT)\n",
    "ax.set_xlabel(\"t-SNE 1\", fontsize=LABEL_SIZE, fontname=FONT)\n",
    "ax.set_ylabel(\"t-SNE 2\", fontsize=LABEL_SIZE, fontname=FONT)\n",
    "\n",
    "# legend\n",
    "leg = ax.legend(title=\"Task\", fontsize=LEGEND_SIZE, title_fontsize=LEGEND_TITLE_SIZE,)\n",
    "for t in leg.get_texts():\n",
    "    t.set_fontname(FONT)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metaworld",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
