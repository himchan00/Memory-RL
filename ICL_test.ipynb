{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851f430b",
   "metadata": {},
   "source": [
    "# A Notebook for Testing In-Context Learning ablility of trained sequencial policies\n",
    "\n",
    "Note that this codebase is made for evaluations in mujoco environments, therefore probably not compatible with other environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78d8e39",
   "metadata": {},
   "source": [
    "## Make environment, Set seed and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "038f5444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himchan/anaconda3/envs/hist/lib/python3.10/site-packages/gymnasium/envs/registration.py:636: UserWarning: \u001b[33mWARN: Overriding environment ant-dir already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register\n",
    "from torchkit.pytorch_utils import set_gpu_mode\n",
    "\n",
    "seed = 42\n",
    "gpu_id = 0\n",
    "device = f'cuda:{gpu_id}'\n",
    "set_gpu_mode(True, gpu_id)\n",
    "\n",
    "\n",
    "ENTRY_POINTS = {\"cheetah-vel\": \"envs.mujoco:HalfCheetahVelEnv\", \"ant-dir\": \"envs.mujoco:AntDirEnv\", \n",
    "                \"hopper-param\": \"envs.mujoco:HopperRandParamsEnv\", \"walker-param\": \"envs.mujoco:Walker2DRandParamsEnv\"}\n",
    "env_name = 'ant-dir'  # Example environment name\n",
    "entry_point = ENTRY_POINTS[env_name]\n",
    "register(\n",
    "    env_name,\n",
    "    entry_point=entry_point,\n",
    "    max_episode_steps=200,\n",
    "    kwargs=dict(terminate_when_unhealthy=True) if env_name not in [\"cheetah-vel\"] else {} # cheetah-vel does not have is_healthy\n",
    ")\n",
    "\n",
    "env = gym.make(env_name)\n",
    "env.max_episode_steps = getattr(\n",
    "    env, \"max_episode_steps\", env.spec.max_episode_steps\n",
    ")\n",
    "env.reset(seed=seed) # Set random seed\n",
    "env.action_space.seed(seed)\n",
    "env.observation_space.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71032235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs space Box(-inf, inf, (105,), float64)\n",
      "act space Box(-1.0, 1.0, (8,), float32)\n",
      "obs_dim 105 act_dim 8\n"
     ]
    }
   ],
   "source": [
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "\n",
    "act_dim = action_space.shape[0]\n",
    "obs_dim = observation_space.shape[0]\n",
    "\n",
    "print(\"obs space\", observation_space)\n",
    "print(\"act space\", action_space)\n",
    "print(\"obs_dim\", obs_dim, \"act_dim\", act_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aee38a2",
   "metadata": {},
   "source": [
    "## Instantiate agent and buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30315ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h.0.ln_1.weight': torch.Size([128]), 'h.0.ln_1.bias': torch.Size([128]), 'h.0.attn.c_attn.weight': torch.Size([128, 384]), 'h.0.attn.c_attn.bias': torch.Size([384]), 'h.0.attn.c_proj.weight': torch.Size([128, 128]), 'h.0.attn.c_proj.bias': torch.Size([128]), 'h.0.ln_2.weight': torch.Size([128]), 'h.0.ln_2.bias': torch.Size([128]), 'h.0.mlp.c_fc.weight': torch.Size([128, 512]), 'h.0.mlp.c_fc.bias': torch.Size([512]), 'h.0.mlp.c_proj.weight': torch.Size([512, 128]), 'h.0.mlp.c_proj.bias': torch.Size([128]), 'ln_f.weight': torch.Size([128]), 'ln_f.bias': torch.Size([128])}\n",
      "Normalize transitions: True\n"
     ]
    }
   ],
   "source": [
    "from policies.models.policy_rnn_shared import ModelFreeOffPolicy_Shared_RNN as Policy_Shared_RNN\n",
    "agent_class = Policy_Shared_RNN\n",
    "from configs.seq_models.gpt_default import get_config as get_config_gpt\n",
    "from configs.seq_models.hist_default import get_config as get_config_hist\n",
    "from configs.seq_models.lstm_default import get_config as get_config_lstm\n",
    "from configs.rl.sac_default import get_config as get_config_rl\n",
    "\n",
    "config_seq = get_config_gpt()\n",
    "config_seq.seq_model.max_seq_length = env.max_episode_steps + 1\n",
    "if config_seq.seq_model.name == \"hist\":\n",
    "    config_seq.seq_model.init_emb_mode = \"parameter\" # For hist\n",
    "config_rl = get_config_rl()\n",
    "\n",
    "agent = agent_class(\n",
    "    obs_dim=obs_dim,\n",
    "    action_dim=act_dim,\n",
    "    config_seq=config_seq,\n",
    "    config_rl=config_rl,\n",
    "    freeze_critic=True,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "from buffers.rollout_buffer import RolloutBuffer\n",
    "buffer = RolloutBuffer(observation_dim=obs_dim,\n",
    "            action_dim=act_dim,\n",
    "            max_episode_len=env.max_episode_steps,\n",
    "            num_episodes=10000, # Not used in ICL testing\n",
    "            normalize_transitions=True,\n",
    "            is_ppo=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91cba5",
   "metadata": {},
   "source": [
    "### Load checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "318b14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "agent_checkpoint_path = \"./logs/mujoco/ant-dir/gpt_1_2025-12-06-22:00:20/policy_checkpoint_latest.pth\"\n",
    "agent.load_state_dict(torch.load(agent_checkpoint_path, map_location=device))\n",
    "buffer_checkpoint_path = \"./logs/mujoco/ant-dir/gpt_1_2025-12-06-22:00:20/buffer_checkpoint_latest.pth\"\n",
    "buffer.load_state_dict(torch.load(buffer_checkpoint_path, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b59e1",
   "metadata": {},
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe80db55",
   "metadata": {},
   "source": [
    "### Define neccessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bf5c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_dummies(env, obs):\n",
    "    prev_obs = obs.clone()\n",
    "    action = torch.FloatTensor([env.action_space.sample()]).to(device).reshape(1, -1)  # (1, A) for continuous action, (1, 1) for discrete action\n",
    "    reward = torch.zeros((1, 1)).to(device)\n",
    "    term = torch.zeros((1, 1)).to(device)\n",
    "    return prev_obs,action,reward,term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96da23ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(internal_state, action, reward, prev_obs, obs, deterministic, initial):\n",
    "    if buffer.normalize_transitions:\n",
    "        obs = buffer.observation_rms.norm(obs)\n",
    "        prev_obs = buffer.observation_rms.norm(prev_obs)\n",
    "        reward = buffer.rewards_rms.norm(reward)\n",
    "    action, internal_state = agent.act(\n",
    "        prev_internal_state=internal_state,\n",
    "        prev_action=action,\n",
    "        prev_reward=reward,\n",
    "        prev_obs=prev_obs,\n",
    "        obs=obs,\n",
    "        deterministic=deterministic,\n",
    "        initial=initial,\n",
    "    )\n",
    "    return action, internal_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff9bfaf",
   "metadata": {},
   "source": [
    "### Vanilla Evaluation Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c0540c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Rollout Test\n",
      "Architecture: gpt\n",
      "Rollout 1/10, Return: 767.9789963147491\n",
      "Rollout 2/10, Return: 483.54009839051014\n",
      "Rollout 3/10, Return: 463.68521597318636\n",
      "Rollout 4/10, Return: 236.57234305020677\n",
      "Rollout 5/10, Return: 642.2710001487184\n",
      "Rollout 6/10, Return: 587.5448901108724\n",
      "Rollout 7/10, Return: 944.7995966988555\n",
      "Rollout 8/10, Return: 1028.2935457978717\n",
      "Rollout 9/10, Return: 746.8665812540876\n",
      "Rollout 10/10, Return: 673.5070801388183\n",
      "Return over 10 rollouts: avg 657.5059347877876, std 221.49697124327716\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_rollouts = 10\n",
    "max_ep_len = env.max_episode_steps\n",
    "returns = []\n",
    "\n",
    "print(\"Vanilla Rollout Test\")\n",
    "print(\"Architecture:\", config_seq.seq_model.name)\n",
    "for rollout_idx in range(num_rollouts):\n",
    "    done = False\n",
    "    ep_return = 0.0\n",
    "    t = 0\n",
    "    obs, info = env.reset()\n",
    "    obs = torch.from_numpy(obs).float().to(device).unsqueeze(0) # (1, obs_dim)\n",
    "    prev_obs, action, reward, term = get_initial_dummies(env, obs)\n",
    "    internal_state = None\n",
    "    initial=True\n",
    "    \n",
    "    while not done and t < max_ep_len:\n",
    "        action, internal_state = act(\n",
    "            internal_state=internal_state,\n",
    "            action=action,\n",
    "            reward=reward,\n",
    "            prev_obs=prev_obs,\n",
    "            obs=obs,\n",
    "            deterministic=True,\n",
    "            initial=initial,\n",
    "        )\n",
    "        initial=False\n",
    "        np_action = action.to(\"cpu\").detach().numpy().squeeze(0) # (act_dim,)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(np_action)\n",
    "        ep_return += reward\n",
    "        next_obs = torch.from_numpy(next_obs).float().to(device).unsqueeze(0)  # (1, obs_dim)\n",
    "        reward = torch.FloatTensor([[reward]]).to(device)  # (1, 1)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        prev_obs = obs.clone()\n",
    "        obs = next_obs.clone()\n",
    "        t += 1\n",
    "\n",
    "    returns.append(ep_return)\n",
    "    print(f\"Rollout {rollout_idx + 1}/{num_rollouts}, Return: {ep_return}\")\n",
    "\n",
    "returns = np.array(returns)\n",
    "print(f\"Return over {num_rollouts} rollouts: avg {np.mean(returns)}, std {np.std(returns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b292ed5f",
   "metadata": {},
   "source": [
    "### In Context Learning Evaluation Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91856304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Context Learning Rollout Test\n",
      "Architecture: gpt\n",
      "Rollout 1/20 (Demo agent), Return: 187.54519195144542\n",
      "Rollout 2/20 (In-Context agent), Return: 58.77346646799395\n",
      "Rollout 3/20 (Demo agent), Return: 652.6070923081727\n",
      "Rollout 4/20 (In-Context agent), Return: 117.84690021426667\n",
      "Rollout 5/20 (Demo agent), Return: 466.12637530942567\n",
      "Rollout 6/20 (In-Context agent), Return: 117.93786456226694\n",
      "Rollout 7/20 (Demo agent), Return: 783.9232823757804\n",
      "Rollout 8/20 (In-Context agent), Return: 85.04459597740666\n",
      "Rollout 9/20 (Demo agent), Return: 353.91509173610245\n",
      "Rollout 10/20 (In-Context agent), Return: 98.979252761565\n",
      "Rollout 11/20 (Demo agent), Return: 1158.353785926193\n",
      "Rollout 12/20 (In-Context agent), Return: 75.37303808055076\n",
      "Rollout 13/20 (Demo agent), Return: 416.1202213205586\n",
      "Rollout 14/20 (In-Context agent), Return: 54.6367613971334\n",
      "Rollout 15/20 (Demo agent), Return: 875.5109840309419\n",
      "Rollout 16/20 (In-Context agent), Return: 122.20902937183098\n",
      "Rollout 17/20 (Demo agent), Return: 835.3530718798594\n",
      "Rollout 18/20 (In-Context agent), Return: 94.48891514225606\n",
      "Rollout 19/20 (Demo agent), Return: 733.1486212372548\n",
      "Rollout 20/20 (In-Context agent), Return: 112.25733906441286\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"In Context Learning Rollout Test\")\n",
    "print(\"Architecture:\", config_seq.seq_model.name)\n",
    "\n",
    "num_rollouts = 10\n",
    "max_ep_len = env.max_episode_steps\n",
    "returns = []\n",
    "\n",
    "total_rollouts = 2 * num_rollouts\n",
    "for rollout_idx in range(total_rollouts):\n",
    "    if rollout_idx % 2 == 0:\n",
    "        keep_context = False\n",
    "    else:\n",
    "        keep_context = True\n",
    "    done = False\n",
    "    ep_return = 0.0\n",
    "    t = 0\n",
    "    obs, info = env.reset(options={'keep_context': keep_context})\n",
    "    obs = torch.from_numpy(obs).float().to(device).unsqueeze(0) # (1, obs_dim)\n",
    "    prev_obs, action, reward, term = get_initial_dummies(env, obs)\n",
    "    if not keep_context:\n",
    "        internal_state = None\n",
    "        initial=True\n",
    "\n",
    "    while not done and t < max_ep_len:\n",
    "        action, new_internal_state = act(\n",
    "            internal_state=internal_state,\n",
    "            action=action,\n",
    "            reward=reward,\n",
    "            prev_obs=prev_obs,\n",
    "            obs=obs,\n",
    "            deterministic=True,\n",
    "            initial=initial,\n",
    "        )\n",
    "        if t == 0 and keep_context:\n",
    "            internal_state = internal_state # Internal state must be preserved at the dummy reset at t=0\n",
    "        else:\n",
    "            internal_state = new_internal_state\n",
    "        initial=False\n",
    "        np_action = action.to(\"cpu\").detach().numpy().squeeze(0) # (act_dim,)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(np_action)\n",
    "        ep_return += reward\n",
    "        next_obs = torch.from_numpy(next_obs).float().to(device).unsqueeze(0)  # (1, obs_dim)\n",
    "        reward = torch.FloatTensor([[reward]]).to(device)  # (1, 1)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        prev_obs = obs.clone()\n",
    "        obs = next_obs.clone()\n",
    "        t += 1\n",
    "\n",
    "    returns.append(ep_return)\n",
    "    if not keep_context:\n",
    "        print(f\"Rollout {rollout_idx + 1}/{total_rollouts} (Demo agent), Return: {ep_return}\")\n",
    "    else:\n",
    "        print(f\"Rollout {rollout_idx + 1}/{total_rollouts} (In-Context agent), Return: {ep_return}\")\n",
    "\n",
    "returns = np.array(returns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
